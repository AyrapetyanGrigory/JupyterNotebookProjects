{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforement Learning (Обучение с подкредплением)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym ## Библиотека, содержащая игры\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим среду игры \"Frozen Lake\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание алгоритма нашей игры:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем таблицу \"действий и состояний\" игры, заполнив ее нулями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "## Так как в игре Froze Lake 4 действия (влево, вправо, вверх, вниз), а количество состояний модели равно 16 (это взято из таблицы),\n",
    "## и у библиотеки \"gym\" есть функции именно для этого( observ = 16, act = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lr = 0.8 ## Learning Rate\n",
    "y = .95 ## это гамма из уравнения Беллмана\n",
    "num_episodes = 2000## Количество сыгранных игр моделью "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список наград, полученных моделью:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим сам алгоритм обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(num_episodes):# от 0 до 20000\n",
    "    ## Сначала расписываем изначальные и обнуленные данные:\n",
    "    s = env.reset() ## Обнуляем первоначальное состояние\n",
    "    rAll = 0 ## Обнуляем награды\n",
    "    d = False ## Означает, что игра не пройдена\n",
    "    j = 0 ## Обнуляем выполненные шаги\n",
    "    ## Создадим цикл шагов:\n",
    "    while j < 99:\n",
    "        ## Совершение шага:\n",
    "        j += 1 \n",
    "        ## Выполнение действия:\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) ##argmax дает максимальное вознаграджение, второе - добавление шума\n",
    "        ## Создадим функцию возвращения переменных:\n",
    "        s1,r,d,_ = env.step(a) ## Мы даем функции значение шага, который она должна выпонить, вернув значения s1(новое состояние), награду за действие(равна 1 в Frozen Lake)(r), пройдена ли игра (d)\n",
    "        ## Обновляем нашу таблицу урванением Беллмана:\n",
    "        Q[s,a] = Q[s,a] + Lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        ## Обновляем количество наград:\n",
    "        rAll += r\n",
    "        ## Обновляем состояние:\n",
    "        s = s1\n",
    "        ## Проверяем, дошли ли мы до конца:\n",
    "        if d == True:\n",
    "            break\n",
    "        ## Добавим наши награды в созданный ранее список rList:\n",
    "        rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим наш алгоритм на предмет того, как он играл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" + str(sum(rList) / num_episodes)) ## Средняя награда за игру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Tab Score: \n",
      "[[2.01985725e-01 2.02704250e-02 2.21917781e-02 2.44543204e-02]\n",
      " [3.14103032e-03 6.98045628e-03 8.34298061e-03 9.80248843e-02]\n",
      " [9.61617535e-03 5.99370060e-03 7.78665018e-03 8.35560737e-02]\n",
      " [1.48095512e-03 1.45789094e-03 6.07319903e-04 5.26545033e-02]\n",
      " [1.86426621e-01 1.12923522e-02 9.43457046e-03 1.89233026e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.62600804e-02 6.08789634e-04 1.51176791e-04 1.04549832e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.11333399e-03 4.26028463e-03 4.90231246e-04 4.16680117e-01]\n",
      " [4.02290038e-05 6.00662761e-01 0.00000000e+00 2.93976104e-04]\n",
      " [8.81096388e-01 1.21085876e-03 1.13189331e-03 4.92150795e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 2.45840732e-02 8.28588684e-01 2.34297343e-02]\n",
      " [0.00000000e+00 9.97162352e-01 0.00000000e+00 4.67554295e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Tab Score: \")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(s1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
